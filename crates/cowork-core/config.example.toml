[llm]
api_base_url = "http://localhost:8000/v1"
api_key = "your-api-key-here"
model_name = "gpt-4"

# Example configurations for different providers:

# OpenAI
# api_base_url = "https://api.openai.com/v1"
# api_key = "sk-..."
# model_name = "gpt-4o"

# DeepSeek
# api_base_url = "https://api.deepseek.com/v1"
# api_key = "sk-..."
# model_name = "deepseek-chat"

# Local Ollama
# api_base_url = "http://localhost:11434/v1"
# api_key = "ollama"
# model_name = "llama3"

# =============================================================================
# External Coding Agent Configuration (ACP Protocol)
# =============================================================================
# Enable external coding agent to use CLI tools like iFlow, Gemini CLI, Codex as
# the underlying coding agent via ACP (Agent Client Protocol).

# [coding_agent]
# enabled = true
# agent_type = "iflow"           # Agent type: "iflow", "codex", "gemini", "claude"
# command = "iflow"              # Command to launch the agent
# args = ["--experimental-acp"]  # Arguments to enable ACP mode
# workspace_path = null          # Optional: override workspace path
# env = { }                      # Optional: environment variables

# Example for iFlow:
# [coding_agent]
# enabled = true
# agent_type = "iflow"
# command = "iflow"
# args = ["--experimental-acp"]

# Example for Gemini CLI:
# [coding_agent]
# enabled = true
# agent_type = "gemini"
# command = "gemini"
# args = []

# Example for Codex CLI:
# [coding_agent]
# enabled = true
# agent_type = "codex"
# command = "codex"
# args = ["exec", "--full-auto"]
