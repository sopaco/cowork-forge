# System Boundary Interface Documentation

This document describes the system's external invocation interfaces, including CLI commands, API endpoints, configuration parameters, and other boundary mechanisms.

## Command Line Interface (CLI)

### cowork-v2 new

**Description**: 启动一个新项目，根据用户提供的项目构想自动生成完整开发流程

**Source File**: `crates/cowork-cli/src/main.rs`

**Arguments**:

- `idea` (string): required - 项目构想或需求描述，用于指导AI生成需求文档、架构设计等内容

**Options**:

- `config, c`(string): optional - 配置文件路径，默认为config.toml
- `verbose, v`(bool): optional - 启用详细日志输出，显示更多调试信息
- `stream, s`(bool): optional - 启用LLM实时流式输出，显示AI思考过程

**Usage Examples**:

```bash
cowork-v2 new "开发一个在线教育平台"
```

```bash
cowork-v2 new "构建一个电商后台管理系统" -c ./myconfig.toml -v
```

```bash
cowork-v2 new "创建一个任务管理应用" --stream
```

## Integration Suggestions

### CLI集成

将Cowork Forge V2作为自动化开发工具集成到CI/CD流水线或开发脚本中，实现一键启动开发流程

**Example Code**:

```
#!/bin/bash
# 自动化启动新项目
PROJECT_NAME="自动化订单系统"

cowork-v2 new "$PROJECT_NAME" --stream --config config.prod.toml

# 检查是否成功生成交付报告
if [ -f ".cowork/artifacts/delivery_report.md" ]; then
    echo "✅ 项目自动生成完成"
    cat .cowork/artifacts/delivery_report.md
else
    echo "❌ 项目生成失败，请检查日志"
    exit 1
fi
```

**Best Practices**:

- 使用环境变量管理API密钥，避免在脚本中硬编码
- 在CI/CD中设置超时机制防止AI流程无限等待
- 将生成的交付报告存档为CI/CD产物供审查
- 集成后应验证.cowork目录结构完整性以确认流程成功

### 配置管理集成

通过环境变量或配置模板动态管理LLM接入参数，实现多环境部署

**Example Code**:

```
# 环境变量方式（推荐用于CI/CD）
export LLM_API_BASE_URL="https://api.openai.com/v1"
export LLM_API_KEY="sk-xxxxxxxx"
export LLM_MODEL_NAME="gpt-4-turbo"

cowork-v2 new "开发CRM系统" --stream

# 或使用配置模板生成
sed "s/your-api-key-here/$LLM_API_KEY/g" config.example.toml > config.toml
cowork-v2 new "开发支付系统" --config config.toml
```

**Best Practices**:

- 使用环境变量而非配置文件管理敏感信息（如API密钥）
- 提供config.example.toml作为模板供团队标准化配置
- 在CI/CD中使用vault或secrets管理器存储API密钥
- 配置文件应包含默认值和验证逻辑以避免运行时错误

### 外部系统调用

通过调用CLI命令触发AI开发流程，实现与其他系统的自动化联动

**Example Code**:

```
import subprocess
import os

def trigger_cowork_development(idea: str, config_path: str = "config.toml") -> str:
    # 设置环境变量
    os.environ["LLM_API_KEY"] = "your-api-key-here"
    
    # 调用CLI命令
    result = subprocess.run([
        "cowork-v2", "new", idea, 
        "--config", config_path, 
        "--stream"
    ], capture_output=True, text=True)
    
    if result.returncode != 0:
        raise Exception(f"Cowork execution failed: {result.stderr}")
    
    # 返回交付报告内容
    with open(".cowork/artifacts/delivery_report.md", "r") as f:
        return f.read()

# 使用示例
report = trigger_cowork_development("开发一个AI客服机器人")
print(report)
```

**Best Practices**:

- 使用子进程调用时应捕获并处理错误输出
- 避免在Web服务中直接同步调用CLI，建议使用异步队列
- 为CLI调用设置合理的超时时间（建议10分钟以上）
- 在调用前后验证.cowork目录是否存在以避免状态冲突


---

**Analysis Confidence**: 9.5/10
