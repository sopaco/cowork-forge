# System Boundary Interface Documentation

This document describes the system's external invocation interfaces, including CLI commands, API endpoints, configuration parameters, and other boundary mechanisms.

## Command Line Interface (CLI)

### cowork-v2

**Description**: AI-powered software development system V2

**Source File**: `crates/cowork-cli-v2/src/main.rs`

**Arguments**:

- `command` (string): required - The subcommand to execute (new, resume, modify, status, init)
- `config` (string): optional - Path to config file (default: config.toml) (default: `config.toml`)
- `verbose` (boolean): optional - Enable verbose logging (default: `false`)
- `stream` (boolean): optional - Enable LLM streaming output (shows AI thinking process in real-time) (default: `false`)

**Usage Examples**:

```bash
cowork-v2 new "Build a task management app"
```

```bash
cowork-v2 resume --verbose
```

```bash
cowork-v2 modify --from design
```

```bash
cowork-v2 status
```

```bash
cowork-v2 init
```

### cowork

**Description**: AI-powered multi-agent software development forge

**Source File**: `crates/cowork-cli/src/main.rs`

**Arguments**:

- `command` (string): required - The subcommand to execute (resume, inspect, export, modify)
- `config` (string): optional - Path to model configuration file (TOML) (default: `config.toml`)

**Usage Examples**:

```bash
cowork resume abc123
```

```bash
cowork inspect abc123
```

```bash
cowork export abc123
```

```bash
cowork modify abc123 --change "Add user authentication"
```

## Integration Suggestions

### CLI Integration

Integrate Cowork Forge into CI/CD pipelines by invoking CLI commands with environment variables for configuration.

**Example Code**:

```
#!/bin/bash
# Example: CI pipeline script
export LLM_API_BASE_URL="https://api.openai.com/v1"
export LLM_API_KEY="sk-..."
export LLM_MODEL_NAME="gpt-4"

cowork-v2 new "Generate a REST API for user management" --stream

if [ $? -eq 0 ]; then
  echo "✅ Project generated successfully"
else
  echo "❌ Project generation failed"
  exit 1
fi
```

**Best Practices**:

- Use environment variables to inject API credentials securely
- Avoid hardcoding secrets in scripts
- Enable streaming mode for debugging in CI logs
- Validate .cowork/ directory exists before proceeding
- Use --verbose mode for detailed error diagnostics

### Configuration Management

Use template-based configuration files and environment variable fallbacks to manage credentials across environments.

**Example Code**:

```
# config.toml template
[llm]
api_base_url = "${LLM_API_BASE_URL:-http://localhost:8000/v1}"
api_key = "${LLM_API_KEY}"
model_name = "${LLM_MODEL_NAME:-gpt-4}"

[embedding]
api_base_url = "${EMBEDDING_API_BASE_URL:-http://localhost:8000/v1}"
api_key = "${EMBEDDING_API_KEY}"
model_name = "${EMBEDDING_MODEL_NAME:-text-embedding-3-small}"
```

**Best Practices**:

- Use .env files for local development
- Use config.toml as the source of truth in production
- Validate required environment variables at startup
- Use configuration validation libraries to prevent malformed configs
- Document all configurable parameters in README.md

### Human-in-the-Loop (HITL) Integration

Extend HITL capabilities by integrating with external feedback systems or ticketing platforms.

**Example Code**:

```
// Pseudocode: Integrate HITL with Jira
use cowork_core::hitl::HitlController;

fn collect_feedback_from_jira(issue_id: &str) -> Result<String> {
    let hitl = HitlController::new();
    
    // Fetch Jira issue description
    let jira_desc = fetch_jira_issue(issue_id)?;
    
    // Allow user to review and edit via editor
    let modified = hitl.review_and_edit_json("Jira Issue", &jira_desc)?;
    
    // If modified, update Jira
    if let Some(text) = modified {
        update_jira_issue(issue_id, &text)?;
    }
    
    Ok(text.unwrap_or(jira_desc))
}
```

**Best Practices**:

- Use HITL tools for human review before critical stages
- Integrate with ticketing systems to track feedback iterations
- Log all HITL interactions for audit trails
- Avoid blocking workflows; provide skip/continue options
- Use structured JSON review to maintain data integrity

### LLM Client Configuration

Configure and secure LLM client connections using environment variables and rate limiting.

**Example Code**:

```
// Example: Setting up LLM client with environment variables
use cowork_core_v2::llm::config::{ModelConfig, create_llm_client};

fn setup_llm_client() -> Result<Arc<dyn Llm>> {
    let config = ModelConfig::from_env()?;
    let client = create_llm_client(&config.llm)?;
    
    println!("Connected to {} at {}", config.llm.model_name, config.llm.api_base_url);
    Ok(client)
}
```

**Best Practices**:

- Always wrap LLM clients with rate limiters
- Use environment variables for API keys, never config files in version control
- Validate API endpoint accessibility before starting workflows
- Implement circuit breaker patterns for LLM outages
- Monitor LLM token usage and cost in production


---

**Analysis Confidence**: 9.5/10
