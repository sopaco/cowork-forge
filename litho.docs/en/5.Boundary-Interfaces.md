# System Boundary Interface Documentation

This document describes the system's external invocation interfaces, including CLI commands, API endpoints, configuration parameters, and other boundary mechanisms.

## Command Line Interface (CLI)

### cowork

**Description**: AI-powered software development system command-line interface

**Source File**: `crates/cowork-cli/src/main.rs`

**Arguments**:

- `config` (string): optional - Path to config file (default: config.toml)
- `verbose` (boolean): optional - Enable verbose logging
- `stream` (boolean): optional - Enable LLM streaming output (shows AI thinking process in real-time)

**Options**:

- `idea`(string): required - Project idea/description
- `from`(string): required - Stage to restart from (prd, design, plan, coding, check, delivery)

**Usage Examples**:

```bash
cowork new "Create a web-based task management application"
```

```bash
cowork resume
```

```bash
cowork modify --from design
```

```bash
cowork status
```

```bash
cowork init
```

```bash
cowork --config custom.toml new "Project idea"
```

```bash
cowork --stream --verbose new "Project with real-time AI output"
```

### create_partial_pipeline

**Description**: Creates pipeline starting from specified stage for targeted workflow execution

**Source File**: `crates/cowork-core/src/pipeline/mod.rs`

**Arguments**:

- `config` (ModelConfig): required - Configuration containing LLM settings
- `start_stage` (string): required - Stage to start pipeline from (prd, design, plan, coding, check, delivery)

**Usage Examples**:

```bash
create_partial_pipeline(config, "design")
```

```bash
create_partial_pipeline(config, "coding")
```

### create_resume_pipeline

**Description**: Intelligently resumes pipeline from appropriate stage based on existing artifacts

**Source File**: `crates/cowork-core/src/pipeline/mod.rs`

**Arguments**:

- `config` (ModelConfig): required - Configuration containing LLM settings

**Usage Examples**:

```bash
create_resume_pipeline(config)
```

## Integration Suggestions

### Configuration Integration

Configuration management integration for LLM API settings

**Example Code**:

```
let config = ModelConfig::from_file("config.toml").unwrap_or_else(|_| ModelConfig::from_env().unwrap());
let llm_client = create_llm_client(&config.llm)?;
```

**Best Practices**:

- Always validate configuration files before execution
- Implement proper error handling for API failures
- Use streaming mode for better user experience with long-running operations
- Set appropriate rate limiting when integrating with external LLM APIs
- Implement proper session management for concurrent users

### Pipeline Orchestration Integration

AI pipeline orchestration integration for software development workflows

**Example Code**:

```
let pipeline = create_cowork_pipeline(&config)?;
let result = execute_pipeline(pipeline, "Project idea description", true).await?;
```

**Best Practices**:

- Implement proper error handling for each pipeline stage
- Use session management for state persistence across pipeline executions
- Monitor pipeline progress with appropriate logging
- Handle partial failures with graceful recovery mechanisms
- Validate artifacts between pipeline stages

### External API Integration

External LLM API integration for AI-powered functionality

**Example Code**:

```
let openai_config = OpenAIConfig::compatible(
    &config.api_key,
    &config.api_base_url,
    &config.model_name
);
```

**Best Practices**:

- Implement proper authentication for API key management
- Use environment variables for sensitive configuration data
- Validate API endpoints before making requests
- Implement retry mechanisms for transient API failures
- Monitor API usage and rate limits


---

**Analysis Confidence**: 9.5/10
