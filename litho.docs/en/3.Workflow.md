# Core Workflows

## 1. Workflow Overview

Cowork Forge is an AI-powered software development system that orchestrates a multi-agent workflow to automate the entire software development lifecycle—from capturing user ideas to delivering production-ready code. The system operates as a CLI-based agent framework that manages project state through session-based persistence and integrates with LLMs via a rate-limited, configurable OpenAI-compatible interface.

### System Main Workflows

The system implements five primary workflows that form the core of its development lifecycle:

1. **Project Initiation and Delivery Lifecycle**: The primary workflow that guides a project from user idea to final delivery through sequential stages: Idea → PRD → Design → Plan → Coding → Check → Delivery.
2. **Change Request Processing and Incremental Modification**: A secondary workflow that handles post-delivery modifications by analyzing change requests and applying targeted updates without restarting the full lifecycle.
3. **Pipeline Resumption and Stage Navigation**: A recovery workflow that enables users to restart or resume the development pipeline from any stage (e.g., PRD, design, coding) to recover from failures or iterate on specific components.
4. **Agent Execution and HITL Recovery**: A foundational workflow that provides Human-in-the-Loop (HITL) capabilities for error recovery during agent execution, particularly when max iteration limits are reached.
5. **LLM Integration and Rate Limiting**: An infrastructure workflow that manages interaction with external LLM providers and ensures compliance with API usage policies.

### Core Execution Paths

The primary execution path follows a linear, sequential pipeline:
1. **Idea Capture** → **PRD Creation** → **System Design** → **Implementation Planning** → **Code Implementation** → **Quality Validation** → **Delivery Verification**

Each stage is governed by an actor-critic agent pair that generates and validates outputs, ensuring simplicity, completeness, and alignment with user intent. The system enforces strict validation rules and requires explicit human approval before advancing, preventing scope creep and ensuring traceability through persistent session artifacts.

### Key Process Nodes

The critical process nodes that define the system's behavior are:

- **Human-in-the-Loop (HITL) Validation Points**: At every major stage (Idea, PRD, Design, Plan, Coding), the system requires explicit human review and approval before proceeding.
- **Simplicity Enforcement Gates**: The PRD Critic, Design Critic, Plan Critic, and Coding Critic agents enforce a strict simplicity-first principle, rejecting non-core requirements, over-engineered architectures, and unnecessary complexity.
- **Validation and Verification Nodes**: The Check Agent performs structural validation of feature coverage and task dependencies, while the Delivery Agent verifies actual code file existence before generating delivery reports.
- **Replanning Triggers**: The Coding Critic can trigger a replanning request when fundamental architectural issues are detected, allowing the system to revert to earlier stages.
- **Stage Navigation Points**: The GotoStageTool enables non-linear progression by allowing users to restart the pipeline from any stage.

### Process Coordination Mechanisms

The system coordinates its workflows through a well-defined orchestration layer:

- **Pipeline Coordinator**: The central nervous system of the system, implemented in `pipeline/mod.rs`, which assembles agent sequences and manages execution flow based on user commands and session state.
- **Session-Based State Management**: All project state is persisted in a hierarchical `.cowork/sessions/<id>/` directory structure, enabling state recovery and non-linear progression.
- **Tool-Based Interaction**: Agents interact with the environment through a set of session-scoped tools (file system, validation, HITL, control) that provide secure, controlled access to system resources.
- **Agent-Critic Architecture**: Each functional stage is implemented as an actor-critic pair, where the Actor generates output and the Critic validates it, creating a built-in quality control mechanism.
- **Event-Driven Execution**: The system uses a sequential agent pattern where each agent completes its task before the next is invoked, creating a deterministic execution flow.

## 2. Main Workflows

### Core Business Process Details

#### Project Initiation and Delivery Lifecycle

This is the primary workflow that guides a project from user idea to final delivery. It follows a strict sequential pipeline with human-in-the-loop validation at each stage.

**Execution Order and Dependencies**:
1. **Idea Agent**: Captures and summarizes the user's initial project idea. This is the entry point triggered by `cowork new`.
2. **PRD Loop**: The PRD Actor generates requirements and features, followed by the PRD Critic validating scope and enforcing simplicity. The Critic rejects non-core features (performance, testing, deployment requirements) and requires human approval via `ReviewWithFeedbackTool`.
3. **Design Loop**: The Design Actor creates a system architecture with 2-4 simple components, followed by the Design Critic validating simplicity and feature coverage. The Critic rejects microservices, complex caching, and message queues unless explicitly required.
4. **Plan Loop**: The Plan Actor decomposes the design into 5-12 implementation tasks, followed by the Plan Critic verifying task simplicity and dependency integrity. The Critic rejects testing, optimization, and deployment tasks.
5. **Coding Loop**: The Coding Actor implements tasks using `WriteFileTool` and `ReadFileTool`, while the Coding Critic validates code quality and task completion. The Critic can trigger `RequestReplanningTool` if fundamental issues are detected.
6. **Check Agent**: Performs structural validation using `CheckFeatureCoverageTool` and `CheckTaskDependenciesTool` to ensure all features are covered and there are no circular dependencies.
7. **Delivery Agent**: Verifies actual code file existence before generating a delivery report. This is a critical gate—task completion alone is insufficient; concrete evidence of implemented code files is required.

**Input/Output Data Flows**:
- **Input**: User's initial project idea (text input)
- **Outputs**: 
  - `idea.md` (structured markdown summary)
  - `prd.md` (Product Requirements Document)
  - `design.md` (System Architecture Document)
  - `implementation_plan.json` (structured task list)
  - Code files in project root
  - `delivery_report.md` (final delivery documentation)

The data flows from one stage to the next through session-scoped storage. Each agent loads the output of the previous stage (e.g., Coding Actor loads the plan from `implementation_plan.json`) and saves its output to the session's artifact directory.

#### Change Request Processing and Incremental Modification

This workflow handles post-delivery modifications by analyzing user change requests and applying targeted updates.

**Execution Order and Dependencies**:
1. **Change Triage Agent**: Analyzes the user's change request to determine scope (PRD, design, plan, code changes). Uses `get_requirements()`, `get_design()`, `get_plan()`, and `list_files()` to understand the current state.
2. **SaveChangeRequestTool**: Persists the scope analysis, risk assessment, and impact analysis as a `change_request.json` file in the session's root directory.
3. **Targeted Pipeline Segment**: Based on the change scope, the system initiates a specialized pipeline:
   - If PRD/design/plan changes are needed: Restart the corresponding stage (e.g., `create_partial_pipeline` with `start_stage="prd"`)
   - If code-only changes: Initiate the Code Patch Pipeline
4. **Code Patch Agent**: Implements incremental changes to existing code files using `WriteFileTool` and `ReadFileTool`, following the ChangeRequest specifications.
5. **Modify Delivery Agent**: Generates a comprehensive change report in pull request-like format, integrating feedback history and session context.

**Input/Output Data Flows**:
- **Input**: User's change request (text input)
- **Outputs**:
  - `change_request.json` (structured scope analysis)
  - Modified code files
  - `delivery_report.md` (change report with PR-style format)

The key innovation is that this workflow operates on a "delta" model—only the affected components are modified, preserving the integrity of the original project while enabling evolutionary development.

#### Pipeline Resumption and Stage Navigation

This workflow enables users to restart or resume the development pipeline from any stage.

**Execution Order and Dependencies**:
1. **GotoStageTool**: Validates the target stage (prd, design, plan, coding) and loads session metadata from `.cowork/sessions/<id>/state/session_meta.json`.
2. **LoadSession**: Restores all artifacts (PRD, design, tasks, feedback, etc.) from the session's artifact and state directories.
3. **Reconstruct Pipeline**: Skips completed stages before the target stage using `create_partial_pipeline`.
4. **Update Session Metadata**: Updates the session's current stage to reflect the new starting point.

**Input/Output Data Flows**:
- **Input**: CLI command with `--goto-stage <stage>` flag
- **Outputs**: Updated session metadata with new starting point

This workflow is critical for iterative refinement and error recovery, allowing users to fix issues at any stage without losing context or prior approvals.

### Key Technical Process Descriptions

#### Agent-Critic Architecture Implementation

The system implements an actor-critic architecture at every major stage, with the following technical implementation:

- **Actor Agents**: Generate output (requirements, design, tasks, code) using LLM prompts defined in `instructions/` directory.
- **Critic Agents**: Validate output using strict rules defined in `instructions/` directory. They use validation tools (`CheckFeatureCoverageTool`, `CheckTaskDependenciesTool`) and file system tools (`read_file`) to verify artifacts.
- **Decision Logic**: Critics use `provide_feedback` to report failures and `request_replanning` for fundamental issues. They never approve output directly—they either reject it or trigger a recovery mechanism.
- **Error Handling**: If a critic rejects output, the pipeline restarts the current stage. This creates a feedback loop that ensures quality.

#### Session-Based State Management

The system uses a hierarchical file-based storage system to manage state:

- **Directory Structure**: `.cowork/sessions/<id>/`
  - `artifacts/`: Markdown documents (idea.md, prd.md, design.md, delivery_report.md)
  - `state/`: Structured data (requirements.json, design_spec.json, implementation_plan.json, session_meta.json)
  - `patch/`: Metadata for incremental changes
  - `logs/`: Execution logs

- **Data Models**: Defined in `data/models.rs`, these are Serde-serializable structs that represent the domain objects (ProjectIdea, PRD, DesignComponent, ImplementationTask, SessionMetadata).

- **Storage Layer**: Implemented in `storage/mod.rs`, this provides CRUD operations for all artifacts. It ensures path security by validating all file paths are relative and within the current working directory.

#### LLM Integration and Rate Limiting

The system integrates with OpenAI-compatible LLMs through a secure, rate-limited interface:

- **Configuration**: Loaded from `config.toml` or environment variables in `llm/config.rs`. Supports custom base URLs for local LLM endpoints.
- **Rate Limiting**: Implemented in `llm/rate_limiter.rs` as a middleware wrapper that adds a 2-second delay before each API call, ensuring compliance with <30 calls per minute limits.
- **Client Creation**: `create_llm_client()` wraps the OpenAIClient from adk-rust with the RateLimitedLlm wrapper, creating a single, secure interface for all agents.

#### Tooling and Security Constraints

All agent interactions with the environment are mediated through a set of secure tools:

- **Path Validation**: All file operations in `file_tools.rs` validate that paths are relative and within the current working directory, preventing directory traversal attacks.
- **Hidden File Filtering**: Tools filter out hidden files (starting with `.`) to avoid system files.
- **Command Execution**: `RunCommandTool` blocks long-running services (e.g., `npm dev`, `python -m http.server`) to prevent accidental server starts.
- **Session Scoping**: All tools operate within the context of a specific session, ensuring isolation between projects.

## 3. Flow Coordination and Control

### Multi-Module Coordination Mechanisms

The system's workflows are coordinated through a well-defined set of module interactions:

1. **Pipeline Coordinator → Intelligent Agent Control**: The pipeline module (`pipeline/mod.rs`) assembles agent sequences and invokes them in order. It loads instruction sets from the `instructions/` directory to configure each agent's behavior.

2. **Pipeline Coordinator → Data & Artifact Management**: The pipeline depends on the storage layer (`storage/mod.rs`) to persist and retrieve session state. Before starting a stage, it checks for existing artifacts (e.g., `has_requirements()` to determine if PRD exists) to determine the resume point.

3. **Pipeline Coordinator → Tooling & Operations**: During agent execution, the pipeline invokes tool functions (file, validation, HITL) to interact with the environment. For example, when the Coding Actor needs to write a file, the pipeline invokes `WriteFileTool`.

4. **Intelligent Agent Control → Tooling & Operations**: Agents rely on tools to perform operations. The PRD Actor uses `create_requirement()` and `add_feature()` to create structured data, and `save_prd_doc()` to persist the document.

5. **Tooling & Operations → Data & Artifact Management**: Tools interact with the storage layer to persist and retrieve artifacts. For example, `SavePrdDocTool` writes to `.cowork/sessions/<id>/artifacts/prd.md` using the storage layer's `save_prd_doc()` function.

6. **Tooling & Operations → Infrastructure Support**: Tools that require LLM interaction (e.g., agent instructions) depend on the LLM configuration and rate limiter to make API calls. The `create_llm_client()` function is called by the pipeline to create a rate-limited LLM client for all agents.

7. **Infrastructure Support → Intelligent Agent Control**: Agent instructions rely on the configured LLM client to generate responses. The rate limiter ensures compliance with API usage limits during agent reasoning.

### State Management and Synchronization

The system implements a robust state management system based on session persistence:

- **State Representation**: All state is represented as structured data models (`ProjectIdea`, `PRD`, `DesignComponent`, `ImplementationTask`, `SessionMetadata`) defined in `data/models.rs`.
- **Persistence**: State is persisted to disk using the storage layer (`storage/mod.rs`) in JSON format for structured data and markdown for documents.
- **Session Isolation**: Each project has its own session directory (`.cowork/sessions/<id>/`), ensuring complete isolation between projects.
- **Session Inheritance**: The `create_resume_pipeline()` function supports session inheritance by copying artifacts from a base session to a new session, enabling branching workflows.
- **Atomic Updates**: All state updates are atomic—each tool call either succeeds or fails, and the system never leaves state in an inconsistent state.

### Data Passing and Sharing

Data flows between stages through a combination of direct loading and tool-based interaction:

- **Direct Loading**: Agents load the output of the previous stage using `load_idea()`, `get_requirements()`, `get_design()`, `get_plan()` functions from the storage layer.
- **Tool-Based Interaction**: Agents use tools to create and modify data:
  - `create_requirement()` and `add_feature()` create structured data in `state/requirements.json` and `state/feature_list.json`
  - `save_prd_doc()` and `save_design_doc()` save markdown documents in `artifacts/`
  - `write_file()` and `read_file()` modify code files in the project root
- **Feedback History**: All feedback from critics and users is stored in `state/feedback_history.json` and loaded by the Modify Delivery Agent to generate comprehensive change reports.

### Execution Control and Scheduling

The system uses a sequential agent pattern for execution control:

- **SequentialAgent**: The core execution engine from the ADK framework, which invokes agents in sequence.
- **Max Iterations**: Each agent is configured with a maximum number of iterations (typically 1) to prevent infinite loops. This is a key design decision to avoid premature termination of LoopAgents.
- **Exit Conditions**: Agents use `provide_feedback()` to signal failures and `request_replanning()` to signal fundamental issues. They do not use `exit_loop()` (which was removed due to a bug in the ADK framework).
- **ResilientStream**: For error recovery, the system wraps agents in a `ResilientStream` that intercepts errors during stream processing and allows human intervention via `GotoStageTool` or `provide_feedback()`.
- **Non-Linear Progression**: The `create_partial_pipeline()` function enables non-linear progression by reconstructing the pipeline from a specified stage, skipping completed steps.

## 4. Exception Handling and Recovery

### Error Detection and Handling

The system employs a multi-layered approach to error detection and handling:

1. **Validation Tools**: Tools like `CheckDataFormatTool`, `CheckFeatureCoverageTool`, and `CheckTaskDependenciesTool` validate data integrity at each stage. They detect missing fields, circular dependencies, and uncovered features.
2. **Critic Agents**: Critic agents (PRD Critic, Design Critic, Plan Critic, Coding Critic) enforce business rules and reject invalid output. They use `provide_feedback()` to report failures with specific details and suggested fixes.
3. **File System Validation**: The `Delivery Agent` verifies actual code file existence before generating a delivery report, preventing false delivery claims.
4. **Path Security**: All file operations in `file_tools.rs` validate that paths are relative and within the current working directory, preventing directory traversal attacks.
5. **LLM Rate Limiting**: The `RateLimitedLlm` middleware detects and prevents API throttling by enforcing a 2-second delay between calls.

### Exception Recovery Mechanisms

The system implements several recovery mechanisms:

1. **Human-in-the-Loop (HITL) Recovery**: When an agent encounters a "Max iterations" error, the `ResilientAgent` wrapper intercepts the error and presents the user with three options:
   - **Retry (reset counter)**: Restart the agent with a fresh counter.
   - **Provide Guidance & Retry**: Allow the user to provide text feedback, which is then used in the next iteration.
   - **Abort**: Terminate the workflow.

2. **Stage Navigation**: The `GotoStageTool` allows users to restart the pipeline from any stage (prd, design, plan, coding). This is the primary recovery mechanism for fundamental issues.

3. **Replanning Requests**: The Coding Critic can trigger a `RequestReplanningTool` when it detects fundamental architectural issues (design flaws, missing dependencies, architecture conflicts, requirement mismatches). This records the issue and allows the Check Agent to trigger a stage navigation.

4. **Task Management**: The Coding Actor can dynamically manage tasks during implementation using `create_task()`, `update_task()`, and `delete_task()` to adjust the plan when new requirements are discovered.

### Fault Tolerance Strategy Design

The system's fault tolerance strategy is based on the following principles:

1. **Fail Fast**: The system fails fast at validation points. If a critic rejects output, the pipeline restarts the current stage rather than continuing with invalid state.
2. **Idempotent Operations**: All tool operations are idempotent. For example, `save_prd_doc()` can be called multiple times with the same content without side effects.
3. **Atomic State Updates**: All state updates are atomic. The storage layer ensures that if a write fails, the previous state is preserved.
4. **Session Isolation**: Each session is completely isolated, preventing cascading failures between projects.
5. **Graceful Degradation**: If an LLM call fails, the system can retry with the same parameters or fall back to human intervention via HITL.

### Failure Retry and Degradation

The system implements a tiered retry and degradation strategy:

1. **Automatic Retry**: The `RateLimitedLlm` middleware automatically retries LLM calls with exponential backoff if rate limits are exceeded.
2. **Manual Retry**: When an agent encounters a "Max iterations" error, the user can choose to retry with a reset counter.
3. **Guided Retry**: When an agent encounters a critical issue, the user can provide feedback, which is then used in the next iteration.
4. **Stage Degradation**: If a stage fails repeatedly, the user can degrade to a previous stage using `GotoStageTool` to fix the root cause.
5. **Fallback to Manual**: If automated recovery fails, the system falls back to manual intervention. The user can edit files directly, then resume the pipeline.

## 5. Key Process Implementation

### Core Algorithm Processes

#### Agent-Critic Decision Algorithm

The core algorithm for agent-critic decision-making is implemented in the `instructions/` directory and executed by the `agents/mod.rs` module:

1. **Actor Generation**: The Actor agent generates output based on its instruction prompt (e.g., `DESIGN_ACTOR_INSTRUCTION`).
2. **Critic Validation**: The Critic agent loads the Actor's output and validates it against a set of rules:
   - **Data Format**: Uses `check_data_format()` to validate JSON schema.
   - **Feature Coverage**: Uses `check_feature_coverage()` to ensure all features are covered.
   - **Task Dependencies**: Uses `check_task_dependencies()` to detect circular dependencies.
   - **Simplicity Check**: Uses hard-coded rules to reject non-core requirements (performance, testing, deployment).
3. **Decision Logic**:
   - If all checks pass: Critic approves and the pipeline proceeds.
   - If checks fail: Critic calls `provide_feedback()` with details and suggested fixes.
   - If fundamental issue: Critic calls `request_replanning()` to trigger a stage navigation.

#### Validation Algorithm

The validation algorithms are implemented in `validation_tools.rs`:

1. **CheckFeatureCoverageTool**:
   - Load all features from `feature_list.json`
   - Load all design components from `design_spec.json`
   - For each feature, check if it is referenced in any component's `related_features`
   - Return list of uncovered features

2. **CheckTaskDependenciesTool**:
   - Load all tasks from `implementation_plan.json`
   - Build a dependency graph using DFS algorithm
   - Detect cycles in the graph
   - Return list of circular dependencies

3. **CheckDataFormatTool**:
   - Load data file (requirements.json, features.json, etc.)
   - Validate required fields (title, description, acceptance_criteria)
   - Validate data types (string, array, etc.)
   - Return list of validation errors

### Data Processing Pipelines

The system implements a series of data processing pipelines that transform raw user input into structured deliverables:

1. **Idea Processing Pipeline**:
   - Input: Raw user text
   - Processing: `IdeaAgent` parses input and creates structured markdown
   - Output: `idea.md`

2. **PRD Processing Pipeline**:
   - Input: `idea.md`
   - Processing: `PRD Actor` generates requirements and features, `PRD Critic` validates scope
   - Output: `prd.md`, `requirements.json`, `feature_list.json`

3. **Design Processing Pipeline**:
   - Input: `prd.md`, `requirements.json`, `feature_list.json`
   - Processing: `Design Actor` creates architecture, `Design Critic` validates simplicity
   - Output: `design.md`, `design_spec.json`

4. **Plan Processing Pipeline**:
   - Input: `design.md`, `design_spec.json`
   - Processing: `Plan Actor` decomposes design into tasks, `Plan Critic` validates task simplicity
   - Output: `implementation_plan.json`

5. **Coding Processing Pipeline**:
   - Input: `implementation_plan.json`
   - Processing: `Coding Actor` implements tasks using `WriteFileTool`, `Coding Critic` validates code quality
   - Output: Code files in project root

6. **Delivery Processing Pipeline**:
   - Input: `implementation_plan.json`, code files
   - Processing: `Delivery Agent` verifies file existence, generates report
   - Output: `delivery_report.md`

### Business Rule Execution

The system enforces business rules through a combination of instruction prompts and validation tools:

1. **Simplicity Enforcement**:
   - **PRD Critic**: Rejects performance, testing, deployment requirements
   - **Design Critic**: Rejects microservices, complex caching, message queues
   - **Plan Critic**: Rejects testing, optimization, deployment tasks
   - **Coding Critic**: Rejects over-engineered code (complex class hierarchies, design patterns)

2. **Completeness Enforcement**:
   - **Delivery Agent**: Requires actual code file existence before generating report
   - **Check Agent**: Requires all features to be covered by design components
   - **Plan Critic**: Requires all tasks to have dependencies and files_to_create

3. **Traceability Enforcement**:
   - All artifacts are linked: Requirements → Features → Tasks → Code files
   - Feedback history is preserved in `feedback_history.json`
   - Change requests are tracked in `change_request.json`

### Technical Implementation Details

#### Secure File System Operations

The `file_tools.rs` implements secure file operations with the following technical details:

```rust
fn validate_path_security(path: &str) -> Result<PathBuf, String> {
    let path_obj = Path::new(path);
    
    // Rule 1: Reject absolute paths
    if path_obj.is_absolute() {
        return Err("Security: Absolute paths are not allowed".to_string());
    }
    
    // Rule 2: Reject parent directory access (..)
    if path.contains("..") {
        return Err("Security: Parent directory access (..) is not allowed".to_string());
    }
    
    // Rule 3: Canonicalize and verify it's within current directory
    let current_dir = std::env::current_dir()?;
    let full_path = current_dir.join(path);
    let canonical_path = full_path.canonicalize().unwrap_or(full_path);
    
    if !canonical_path.starts_with(&current_dir) {
        return Err("Security: Path escapes current directory".to_string());
    }
    
    Ok(canonical_path)
}
```

This ensures that all file operations are confined to the current working directory and cannot access system files.

#### Rate Limiting Middleware

The `rate_limiter.rs` implements a rate limiting middleware with the following technical details:

```rust
#[async_trait]
impl Llm for RateLimitedLlm {
    async fn generate_content(
        &self,
        req: LlmRequest,
        stream: bool,
    ) -> adk_core::Result<LlmResponseStream> {
        // Wait before making the API call
        sleep(Duration::from_millis(self.delay_ms)).await;
        
        // Delegate to the inner LLM
        self.inner.generate_content(req, stream).await
    }
}
```

This ensures that the system makes no more than 30 calls per minute, preventing API throttling.

#### HITL Recovery Mechanism

The `hitl.rs` implements a resilient agent wrapper with the following technical details:

```rust
enum StreamState {
    Streaming(AgentOutput),
    Retrying(Pin<Box<dyn Future<Output = Result<AgentOutput, AdkError>> + Send>>),
}

struct ResilientStream {
    inner_agent: Arc<dyn Agent>,
    context: Arc<dyn InvocationContext>,
    state: StreamState,
    agent_name: String,
}

impl Stream for ResilientStream {
    fn poll_next(mut self: Pin<&mut Self>, cx: &mut TaskContext<'_>) -> Poll<Option<Self::Item>> {
        loop {
            match &mut self.state {
                StreamState::Streaming(stream) => {
                    match stream.as_mut().poll_next(cx) {
                        Poll::Ready(Some(Err(e))) => {
                            // Intercept error
                            let err_msg = e.to_string();
                            if err_msg.contains(\"Max iterations\") {
                                // Present user with options
                                let selection = Select::with_theme(&ColorfulTheme::default())
                                    .with_prompt(\"How would you like to proceed?\")
                                    .default(0)
                                    .items(&[\"Retry (reset counter)\", \"Provide Guidance & Retry\", \"Abort\"])
                                    .interact().unwrap_or(2);
                                
                                match selection {
                                    0 => { self.start_retry(); continue; }
                                    1 => { self.start_retry(); continue; }
                                    _ => return Poll::Ready(Some(Err(e)));
                                }
                            }
                            return Poll::Ready(Some(Err(e)));
                        },
                        Poll::Ready(other) => return Poll::Ready(other),
                        Poll::Pending => return Poll::Pending,
                    }
                },
                StreamState::Retrying(fut) => {
                    match fut.as_mut().poll(cx) {
                        Poll::Ready(Ok(new_stream)) => {
                            self.state = StreamState::Streaming(new_stream);
                            continue;
                        },
                        Poll::Ready(Err(e)) => return Poll::Ready(Some(Err(e))),
                        Poll::Pending => return Poll::Pending,
                    }
                }
            }
        }
    }
}
```

This provides a seamless user experience for error recovery, allowing users to intervene at any point in the workflow.

#### Session State Management

The `storage/mod.rs` implements session state management with the following technical details:

```rust
pub fn get_session_dir(session_id: &str) -> Result<PathBuf> {
    let cowork_dir = get_cowork_dir()?;
    let session_path = cowork_dir.join(SESSIONS_DIR).join(session_id);
    
    // Create session subdirectories
    fs::create_dir_all(&session_path)?;
    fs::create_dir_all(session_path.join(\"artifacts\"))?;
    fs::create_dir_all(session_path.join(\"state\"))?;
    fs::create_dir_all(session_path.join(\"patch\"))?;
    fs::create_dir_all(session_path.join(\"logs\"))?;
    
    Ok(session_path)
}
```

This creates a clean, isolated directory structure for each session, ensuring that projects are completely independent.

#### Tool Interface Implementation

All tools implement the `Tool` trait from the ADK framework:

```rust
#[async_trait]
impl Tool for SavePrdDocTool {
    fn name(&self) -> &str {
        \"save_prd_doc\"
    }

    fn description(&self) -> &str {
        \"Save the PRD (Product Requirements Document) markdown file.\"
    }

    fn parameters_schema(&self) -> Option<Value> {
        Some(json!({
            \"type\": \"object\",
            \"properties\": {
                \"content\": {
                    \"type\": \"string\",
                    \"description\": \"Markdown content of the PRD document\"
                }
            },
            \"required\": [\"content\"]
        }))
    }

    async fn execute(&self, _ctx: Arc<dyn ToolContext>, args: Value) -> adk_core::Result<Value> {
        let content = args[\"content\"].as_str().ok_or_else(|| adk_core::AdkError::Tool(\"Missing 'content' parameter\".to_string()))?;
        
        save_prd_doc(&self.session_id, content)
            .map_err(|e| adk_core::AdkError::Tool(e.to_string()))?;
        
        Ok(json!({
            \"status\": \"success\",
            \"message\": \"PRD document saved successfully\"
        }))
    }
}
```

This provides a consistent interface for all tools, making it easy to add new functionality and ensuring that all tools are properly documented and validated.